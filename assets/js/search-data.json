{
  
    
        "post0": {
            "title": "Let's see what the computer sees",
            "content": "Neurons for Vision . Here we will be using the Fashion MNIST Dataset for experimentation. Each of our images is a set of 784 values (28 × 28) between 0 and 255. They can be our X. We know that we have 10 different types of images in our dataset, so let’s con‐ sider them to be our Y. Now we want to learn what the function looks like where Y is a function of X. . Exploring the Data . import tensorflow as tf import matplotlib.pyplot as plt data = tf.keras.datasets.fashion_mnist . (train_images,train_labels),(test_images,test_labels) = data.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz 32768/29515 [=================================] - 0s 1us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz 26427392/26421880 [==============================] - 3s 0us/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz 8192/5148 [===============================================] - 0s 0s/step Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz 4423680/4422102 [==============================] - 1s 0us/step . plt.imshow(train_images[20]) plt.show() . Normalizing the Data . train_images = train_images/255.0 test_images = test_images/255.0 . Neuron Architecture . from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense,Flatten . Flatten - Turns a 2D array into 1D in this case, making 28*28 into 784. | Relu - Activation function that outputs values either 0 or greater than zero. | Softmax - Used in cases of multiclass classification since it outputs only the most likely probability | Metrics - Way to measure our progress | . model = Sequential([ Flatten(input_shape=(28,28)), Dense(128,activation=&#39;relu&#39;), Dense(10,activation=&#39;softmax&#39;) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(train_images,train_labels,epochs=10) . Train on 60000 samples Epoch 1/10 60000/60000 [==============================] - 4s 59us/sample - loss: 0.5013 - accuracy: 0.8235 Epoch 2/10 60000/60000 [==============================] - 3s 56us/sample - loss: 0.3754 - accuracy: 0.8647 Epoch 3/10 60000/60000 [==============================] - 3s 56us/sample - loss: 0.3367 - accuracy: 0.8764 Epoch 4/10 60000/60000 [==============================] - 3s 55us/sample - loss: 0.3132 - accuracy: 0.8843 Epoch 5/10 60000/60000 [==============================] - 3s 56us/sample - loss: 0.2937 - accuracy: 0.8916 Epoch 6/10 60000/60000 [==============================] - 3s 57us/sample - loss: 0.2822 - accuracy: 0.8963 Epoch 7/10 60000/60000 [==============================] - 3s 56us/sample - loss: 0.2691 - accuracy: 0.9002 Epoch 8/10 60000/60000 [==============================] - 3s 56us/sample - loss: 0.2588 - accuracy: 0.9031 Epoch 9/10 60000/60000 [==============================] - 3s 58us/sample - loss: 0.2488 - accuracy: 0.9074s - l Epoch 10/10 60000/60000 [==============================] - 3s 57us/sample - loss: 0.2399 - accuracy: 0.9098 . &lt;tensorflow.python.keras.callbacks.History at 0x1c64d3f0240&gt; . . Evaluating . model.evaluate(test_images,test_labels) . 10000/10000 [==============================] - ETA: 0s - loss: 0.3362 - accuracy: 0.88 - 0s 39us/sample - loss: 0.3351 - accuracy: 0.8800 . [0.3351069624185562, 0.88] . Training set accuracy: [90.98%] | Test set accuracy : [88%] | . Exploring the Model Output . classifications = model.predict(test_images) print(classifications[0]) print(test_labels[0]) . [1.4614162e-07 3.7705106e-09 2.0430624e-09 2.1551809e-09 9.3017638e-10 4.5569852e-04 4.8079949e-07 7.8137349e-03 3.7059172e-07 9.9172956e-01] 9 . You’ll notice that the classification gives us back an array of values. These are the val‐ ues of the 10 output neurons. The label is the actual label for the item of clothing, in this case 9. Take a look through the array—you’ll see that some of the values are very small, and the last one (array index 9) is the largest by far. These are the probabilities that the image matches the label at that particular index. So, what the neural network is reporting is that there’s a 91.4% chance that the item of clothing at index 0 is label 9. We know that it’s label 9, so it got it right. . Training for longer - Discovering Overfitting . Here we will increase the no. of epochs to 50 &amp; see how it helps in the accuracies . model = Sequential([ Flatten(input_shape=(28,28)), Dense(128,activation=&#39;relu&#39;), Dense(10,activation=&#39;softmax&#39;) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(train_images,train_labels,epochs=100) . Train on 60000 samples Epoch 1/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.4981 - accuracy: 0.8234 Epoch 2/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.3710 - accuracy: 0.8665 Epoch 3/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.3333 - accuracy: 0.8783 Epoch 4/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.3132 - accuracy: 0.8850 Epoch 5/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.2953 - accuracy: 0.8904s - loss: 0.2958 - accuracy: Epoch 6/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2811 - accuracy: 0.8960 Epoch 7/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2694 - accuracy: 0.9003 Epoch 8/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.2584 - accuracy: 0.9038 Epoch 9/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2483 - accuracy: 0.9077 Epoch 10/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2397 - accuracy: 0.9101 Epoch 11/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.2314 - accuracy: 0.9126 Epoch 12/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.2249 - accuracy: 0.9148 Epoch 13/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2183 - accuracy: 0.9182s - loss: 0.2185 - accuracy: Epoch 14/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.2123 - accuracy: 0.9201 Epoch 15/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.2048 - accuracy: 0.9227 Epoch 16/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.1996 - accuracy: 0.9239s - loss: 0.1996 - accuracy: Epoch 17/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.1962 - accuracy: 0.9260 Epoch 18/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.1895 - accuracy: 0.9281 Epoch 19/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.1847 - accuracy: 0.9300 Epoch 20/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.1803 - accuracy: 0.9319 Epoch 21/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.1764 - accuracy: 0.9326 Epoch 22/100 60000/60000 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.93 - 3s 54us/sample - loss: 0.1700 - accuracy: 0.9365 Epoch 23/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.1651 - accuracy: 0.9377s - loss: 0.1649 - accuracy Epoch 24/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.1631 - accuracy: 0.9387 Epoch 25/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.1571 - accuracy: 0.9399 Epoch 26/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.1543 - accuracy: 0.9422 Epoch 27/100 60000/60000 [==============================] - 4s 58us/sample - loss: 0.1538 - accuracy: 0.9419 Epoch 28/100 60000/60000 [==============================] - 4s 59us/sample - loss: 0.1482 - accuracy: 0.9444s - loss: Epoch 29/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1431 - accuracy: 0.9464 Epoch 30/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1422 - accuracy: 0.9463 Epoch 31/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.1397 - accuracy: 0.9463 Epoch 32/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.1366 - accuracy: 0.9482 Epoch 33/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.1349 - accuracy: 0.9489 Epoch 34/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.1306 - accuracy: 0.9491s - loss: 0.1 - ETA: 0s - loss: 0.1319 Epoch 35/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1274 - accuracy: 0.9517 Epoch 36/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1259 - accuracy: 0.9520 Epoch 37/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1237 - accuracy: 0.9541 Epoch 38/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1216 - accuracy: 0.9534 Epoch 39/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1184 - accuracy: 0.9549 Epoch 40/100 60000/60000 [==============================] - 3s 56us/sample - loss: 0.1175 - accuracy: 0.9558 Epoch 41/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1153 - accuracy: 0.9569 Epoch 42/100 60000/60000 [==============================] - 4s 58us/sample - loss: 0.1119 - accuracy: 0.9582 Epoch 43/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.1093 - accuracy: 0.9586 Epoch 44/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1102 - accuracy: 0.9586s - loss: 0 Epoch 45/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1048 - accuracy: 0.9606 Epoch 46/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1049 - accuracy: 0.9604 Epoch 47/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1028 - accuracy: 0.9603 Epoch 48/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.1038 - accuracy: 0.9605 Epoch 49/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.0984 - accuracy: 0.9633 Epoch 50/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0980 - accuracy: 0.9627 Epoch 51/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0947 - accuracy: 0.9642 Epoch 52/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0943 - accuracy: 0.9638 Epoch 53/100 60000/60000 [==============================] - 4s 58us/sample - loss: 0.0904 - accuracy: 0.9656 Epoch 54/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0908 - accuracy: 0.9663 Epoch 55/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.0891 - accuracy: 0.9663 Epoch 56/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0886 - accuracy: 0.9665 Epoch 57/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0876 - accuracy: 0.9674 Epoch 58/100 60000/60000 [==============================] - 4s 60us/sample - loss: 0.0859 - accuracy: 0.9676 Epoch 59/100 60000/60000 [==============================] - 4s 60us/sample - loss: 0.0849 - accuracy: 0.9682 Epoch 60/100 60000/60000 [==============================] - 3s 58us/sample - loss: 0.0834 - accuracy: 0.9686 Epoch 61/100 60000/60000 [==============================] - 4s 60us/sample - loss: 0.0815 - accuracy: 0.9690 Epoch 62/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0813 - accuracy: 0.9699 Epoch 63/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0799 - accuracy: 0.9699 Epoch 64/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0813 - accuracy: 0.9690 Epoch 65/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0778 - accuracy: 0.9711s - loss: 0.0779 - ac - ETA: 0s - loss: 0.0777 - accuracy: Epoch 66/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0738 - accuracy: 0.9724 Epoch 67/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0772 - accuracy: 0.9708 Epoch 68/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0739 - accuracy: 0.9724 Epoch 69/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0738 - accuracy: 0.9723 Epoch 70/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0735 - accuracy: 0.9728 Epoch 71/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0696 - accuracy: 0.9735 Epoch 72/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0703 - accuracy: 0.9730s - loss: 0.0699 - accuracy: Epoch 73/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0691 - accuracy: 0.9732 Epoch 74/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0677 - accuracy: 0.9750 Epoch 75/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0701 - accuracy: 0.9740 Epoch 76/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0681 - accuracy: 0.9747 Epoch 77/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0660 - accuracy: 0.9750 Epoch 78/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0652 - accuracy: 0.9759 Epoch 79/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0636 - accuracy: 0.9762s - loss: 0.0632 - accuracy: 0.97 Epoch 80/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0667 - accuracy: 0.9747 Epoch 81/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0622 - accuracy: 0.9773s - loss: 0.0623 - accuracy: 0. Epoch 82/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.0635 - accuracy: 0.9765 Epoch 83/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.0601 - accuracy: 0.9778s - loss: 0.0600 - accuracy: 0.97 Epoch 84/100 60000/60000 [==============================] - 3s 55us/sample - loss: 0.0607 - accuracy: 0.9772 Epoch 85/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0610 - accuracy: 0.9774 Epoch 86/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0587 - accuracy: 0.9777 Epoch 87/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0609 - accuracy: 0.9775 Epoch 88/100 60000/60000 [==============================] - 3s 49us/sample - loss: 0.0575 - accuracy: 0.9789 Epoch 89/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.0557 - accuracy: 0.9789 Epoch 90/100 60000/60000 [==============================] - 3s 57us/sample - loss: 0.0581 - accuracy: 0.9776 Epoch 91/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.0551 - accuracy: 0.9790 Epoch 92/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0558 - accuracy: 0.9783 Epoch 93/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0534 - accuracy: 0.9798 Epoch 94/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.0538 - accuracy: 0.9800 Epoch 95/100 60000/60000 [==============================] - 3s 53us/sample - loss: 0.0531 - accuracy: 0.9803 Epoch 96/100 60000/60000 [==============================] - 3s 51us/sample - loss: 0.0538 - accuracy: 0.9800 Epoch 97/100 60000/60000 [==============================] - 3s 52us/sample - loss: 0.0523 - accuracy: 0.9804 Epoch 98/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0519 - accuracy: 0.9812 Epoch 99/100 60000/60000 [==============================] - 3s 50us/sample - loss: 0.0518 - accuracy: 0.9809 Epoch 100/100 60000/60000 [==============================] - 3s 54us/sample - loss: 0.0518 - accuracy: 0.9803 . &lt;tensorflow.python.keras.callbacks.History at 0x1c64bb7b320&gt; . . model.evaluate(test_images,test_labels) . 10000/10000 [==============================] - 0s 39us/sample - loss: 0.4944 - accuracy: 0.8847 . [0.4943554855763912, 0.8847] . Training set accuracy: [98.03%] | Test set accuracy : [88.47%] | . Stopping Training . In each of the cases so far, we’ve hardcoded the number of epochs we’re training for. While that works, we might want to train until we reach the desired accuracy instead of constantly trying different numbers of epochs and training and retraining until we get to our desired value. So, for example, if we want to train until the model is at 95% accuracy on the training set, without knowing how many epochs that will take, how could we do that? The easiest approach is to use a callback on the training. Let’s take a look at the upda‐ ted code that uses callbacks: . class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self,epoch,logs={}): if(logs.get(&#39;accuracy&#39;)&gt;0.95): print(&quot; nReached 95% accuracy, stopping...&quot;) self.model.stop_training=True callbacks = myCallback() model = Sequential([ Flatten(input_shape=(28,28)), Dense(128,activation=&#39;relu&#39;), Dense(10,activation=&#39;softmax&#39;) ]) model.compile(optimizer=&#39;adam&#39;, loss=&#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(train_images,train_labels,callbacks=[callbacks],epochs=50) . Train on 60000 samples Epoch 1/50 60000/60000 [==============================] - 11s 180us/sample - loss: 0.5009 - accuracy: 0.8245 Epoch 2/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.3758 - accuracy: 0.8641 Epoch 3/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.3380 - accuracy: 0.8771 Epoch 4/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.3151 - accuracy: 0.8839 Epoch 5/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2955 - accuracy: 0.8916 Epoch 6/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.2812 - accuracy: 0.8954 Epoch 7/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2667 - accuracy: 0.9013 Epoch 8/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2558 - accuracy: 0.9039s - loss: 0.2 - ETA: 2s - loss: 0.2500 - ETA: 1s - loss: 0.2535 - - ETA: 0s - loss: 0.2538 - accu Epoch 9/50 60000/60000 [==============================] - 3s 48us/sample - loss: 0.2464 - accuracy: 0.9084 Epoch 10/50 60000/60000 [==============================] - 3s 48us/sample - loss: 0.2387 - accuracy: 0.9103 Epoch 11/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2289 - accuracy: 0.9139 Epoch 12/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2216 - accuracy: 0.9180 Epoch 13/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.2152 - accuracy: 0.9197s - l Epoch 14/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.2074 - accuracy: 0.9223 Epoch 15/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.2021 - accuracy: 0.9236 Epoch 16/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1950 - accuracy: 0.9273s - - ETA: 0s - loss: Epoch 17/50 60000/60000 [==============================] - 3s 46us/sample - loss: 0.1896 - accuracy: 0.9289s - loss: 0.1897 - accuracy: 0.92 Epoch 18/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1860 - accuracy: 0.9296s - loss: 0.1825 - accu - ETA Epoch 19/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1790 - accuracy: 0.9323 Epoch 20/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.1763 - accuracy: 0.9337s - loss: 0.1 - ETA: 0s - loss: 0.1772 - accura Epoch 21/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1711 - accuracy: 0.9357 Epoch 22/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1675 - accuracy: 0.9376 Epoch 23/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1618 - accuracy: 0.9386 Epoch 24/50 60000/60000 [==============================] - 3s 50us/sample - loss: 0.1606 - accuracy: 0.9393 Epoch 25/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1554 - accuracy: 0.9406 Epoch 26/50 60000/60000 [==============================] - 3s 47us/sample - loss: 0.1530 - accuracy: 0.9430 Epoch 27/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1499 - accuracy: 0.9439A: Epoch 28/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1440 - accuracy: 0.9467s - loss: 0 - ETA: 0s Epoch 29/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1418 - accuracy: 0.9468s - loss: 0.1417 - accuracy: 0.94 Epoch 30/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1414 - accuracy: 0.9466s - loss: Epoch 31/50 60000/60000 [==============================] - 3s 49us/sample - loss: 0.1358 - accuracy: 0.9487 Epoch 32/50 60000/60000 [==============================] - 3s 48us/sample - loss: 0.1328 - accuracy: 0.9499 Epoch 33/50 59520/60000 [============================&gt;.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9511 ETA: 2s - loss: 0.117 Reached 95% accuracy, stopping... 60000/60000 [==============================] - 3s 50us/sample - loss: 0.1291 - accuracy: 0.9510 . &lt;tensorflow.python.keras.callbacks.History at 0x1c6689792e8&gt; . . model.evaluate(test_images,test_labels) . 10000/10000 [==============================] - 0s 43us/sample - loss: 0.4173 - accuracy: 0.8918 . [0.41726961513757704, 0.8918] . See how early stopping gave better results on test data. .",
            "url": "https://astronaut99.github.io/ml-blog/2021/09/29/CV.html",
            "relUrl": "/2021/09/29/CV.html",
            "date": " • Sep 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Hello World - says the Neuron!",
            "content": "import tensorflow as tf from tensorflow.keras import Sequential from tensorflow.keras.layers import Dense import numpy as np . Let&#39;s try and build a typical neural network . X = np.array([-1,0,1,2,3,4],dtype=float) y = np.array([-3,-1,1,3,5,7],dtype=float) X,y . (array([-1., 0., 1., 2., 3., 4.]), array([-3., -1., 1., 3., 5., 7.])) . Adding a Dense layer with a single neuron &amp; a raw matrix . layer0 = Dense(units=1,input_shape=[1]) model = Sequential([layer0]) print(model.predict([10.0])) . [[-16.16105]] . Compiling the model with a &#39;Stochastic Gradient Descent&#39; Optimizer, where Stochastic -&gt; Random. And Gradient Descent is a mathematical way for the model to reach the Global Minimum. | Calculating the loss by taking the difference of squares of the actual &amp; predicted values. | . model.compile(optimizer=&#39;sgd&#39;,loss=&#39;mean_squared_error&#39;) model.fit(X,y,epochs=250) . Train on 6 samples Epoch 1/250 6/6 [==============================] - 0s 40ms/sample - loss: 0.0015 Epoch 2/250 6/6 [==============================] - 0s 333us/sample - loss: 0.0015 Epoch 3/250 6/6 [==============================] - 0s 506us/sample - loss: 0.0015 Epoch 4/250 6/6 [==============================] - 0s 528us/sample - loss: 0.0014 Epoch 5/250 6/6 [==============================] - 0s 544us/sample - loss: 0.0014 Epoch 6/250 6/6 [==============================] - 0s 692us/sample - loss: 0.0014 Epoch 7/250 6/6 [==============================] - 0s 673us/sample - loss: 0.0014 Epoch 8/250 6/6 [==============================] - 0s 449us/sample - loss: 0.0013 Epoch 9/250 6/6 [==============================] - 0s 338us/sample - loss: 0.0013 Epoch 10/250 6/6 [==============================] - 0s 600us/sample - loss: 0.0013 Epoch 11/250 6/6 [==============================] - 0s 338us/sample - loss: 0.0012 Epoch 12/250 6/6 [==============================] - 0s 534us/sample - loss: 0.0012 Epoch 13/250 6/6 [==============================] - 0s 332us/sample - loss: 0.0012 Epoch 14/250 6/6 [==============================] - 0s 509us/sample - loss: 0.0012 Epoch 15/250 6/6 [==============================] - 0s 402us/sample - loss: 0.0011 Epoch 16/250 6/6 [==============================] - 0s 584us/sample - loss: 0.0011 Epoch 17/250 6/6 [==============================] - 0s 553us/sample - loss: 0.0011 Epoch 18/250 6/6 [==============================] - 0s 508us/sample - loss: 0.0011 Epoch 19/250 6/6 [==============================] - 0s 406us/sample - loss: 0.0011 Epoch 20/250 6/6 [==============================] - 0s 523us/sample - loss: 0.0010 Epoch 21/250 6/6 [==============================] - 0s 792us/sample - loss: 0.0010 Epoch 22/250 6/6 [==============================] - 0s 499us/sample - loss: 9.9368e-04 Epoch 23/250 6/6 [==============================] - 0s 499us/sample - loss: 9.7327e-04 Epoch 24/250 6/6 [==============================] - 0s 607us/sample - loss: 9.5328e-04 Epoch 25/250 6/6 [==============================] - 0s 311us/sample - loss: 9.3370e-04 Epoch 26/250 6/6 [==============================] - 0s 333us/sample - loss: 9.1452e-04 Epoch 27/250 6/6 [==============================] - 0s 610us/sample - loss: 8.9574e-04 Epoch 28/250 6/6 [==============================] - 0s 497us/sample - loss: 8.7734e-04 Epoch 29/250 6/6 [==============================] - 0s 333us/sample - loss: 8.5931e-04 Epoch 30/250 6/6 [==============================] - 0s 333us/sample - loss: 8.4166e-04 Epoch 31/250 6/6 [==============================] - 0s 697us/sample - loss: 8.2437e-04 Epoch 32/250 6/6 [==============================] - 0s 339us/sample - loss: 8.0744e-04 Epoch 33/250 6/6 [==============================] - 0s 553us/sample - loss: 7.9086e-04 Epoch 34/250 6/6 [==============================] - 0s 432us/sample - loss: 7.7461e-04 Epoch 35/250 6/6 [==============================] - 0s 586us/sample - loss: 7.5870e-04 Epoch 36/250 6/6 [==============================] - 0s 509us/sample - loss: 7.4311e-04 Epoch 37/250 6/6 [==============================] - 0s 550us/sample - loss: 7.2785e-04 Epoch 38/250 6/6 [==============================] - 0s 297us/sample - loss: 7.1290e-04 Epoch 39/250 6/6 [==============================] - 0s 332us/sample - loss: 6.9826e-04 Epoch 40/250 6/6 [==============================] - 0s 340us/sample - loss: 6.8392e-04 Epoch 41/250 6/6 [==============================] - 0s 332us/sample - loss: 6.6987e-04 Epoch 42/250 6/6 [==============================] - 0s 172us/sample - loss: 6.5611e-04 Epoch 43/250 6/6 [==============================] - 0s 341us/sample - loss: 6.4263e-04 Epoch 44/250 6/6 [==============================] - 0s 361us/sample - loss: 6.2943e-04 Epoch 45/250 6/6 [==============================] - 0s 179us/sample - loss: 6.1650e-04 Epoch 46/250 6/6 [==============================] - 0s 200us/sample - loss: 6.0384e-04 Epoch 47/250 6/6 [==============================] - 0s 379us/sample - loss: 5.9144e-04 Epoch 48/250 6/6 [==============================] - 0s 333us/sample - loss: 5.7928e-04 Epoch 49/250 6/6 [==============================] - 0s 333us/sample - loss: 5.6739e-04 Epoch 50/250 6/6 [==============================] - 0s 370us/sample - loss: 5.5573e-04 Epoch 51/250 6/6 [==============================] - 0s 167us/sample - loss: 5.4432e-04 Epoch 52/250 6/6 [==============================] - 0s 333us/sample - loss: 5.3314e-04 Epoch 53/250 6/6 [==============================] - 0s 332us/sample - loss: 5.2219e-04 Epoch 54/250 6/6 [==============================] - 0s 338us/sample - loss: 5.1146e-04 Epoch 55/250 6/6 [==============================] - 0s 525us/sample - loss: 5.0095e-04 Epoch 56/250 6/6 [==============================] - 0s 349us/sample - loss: 4.9066e-04 Epoch 57/250 6/6 [==============================] - 0s 353us/sample - loss: 4.8058e-04 Epoch 58/250 6/6 [==============================] - 0s 373us/sample - loss: 4.7071e-04 Epoch 59/250 6/6 [==============================] - 0s 379us/sample - loss: 4.6104e-04 Epoch 60/250 6/6 [==============================] - 0s 505us/sample - loss: 4.5157e-04 Epoch 61/250 6/6 [==============================] - 0s 338us/sample - loss: 4.4230e-04 Epoch 62/250 6/6 [==============================] - 0s 346us/sample - loss: 4.3321e-04 Epoch 63/250 6/6 [==============================] - 0s 332us/sample - loss: 4.2431e-04 Epoch 64/250 6/6 [==============================] - 0s 504us/sample - loss: 4.1560e-04 Epoch 65/250 6/6 [==============================] - 0s 500us/sample - loss: 4.0707e-04 Epoch 66/250 6/6 [==============================] - 0s 498us/sample - loss: 3.9870e-04 Epoch 67/250 6/6 [==============================] - 0s 209us/sample - loss: 3.9051e-04 Epoch 68/250 6/6 [==============================] - 0s 332us/sample - loss: 3.8249e-04 Epoch 69/250 6/6 [==============================] - 0s 577us/sample - loss: 3.7464e-04 Epoch 70/250 6/6 [==============================] - 0s 500us/sample - loss: 3.6694e-04 Epoch 71/250 6/6 [==============================] - 0s 333us/sample - loss: 3.5940e-04 Epoch 72/250 6/6 [==============================] - 0s 332us/sample - loss: 3.5202e-04 Epoch 73/250 6/6 [==============================] - 0s 340us/sample - loss: 3.4479e-04 Epoch 74/250 6/6 [==============================] - 0s 332us/sample - loss: 3.3771e-04 Epoch 75/250 6/6 [==============================] - 0s 864us/sample - loss: 3.3077e-04 Epoch 76/250 6/6 [==============================] - 0s 333us/sample - loss: 3.2398e-04 Epoch 77/250 6/6 [==============================] - 0s 166us/sample - loss: 3.1732e-04 Epoch 78/250 6/6 [==============================] - 0s 514us/sample - loss: 3.1080e-04 Epoch 79/250 6/6 [==============================] - 0s 564us/sample - loss: 3.0442e-04 Epoch 80/250 6/6 [==============================] - 0s 332us/sample - loss: 2.9817e-04 Epoch 81/250 6/6 [==============================] - 0s 166us/sample - loss: 2.9204e-04 Epoch 82/250 6/6 [==============================] - 0s 186us/sample - loss: 2.8604e-04 Epoch 83/250 6/6 [==============================] - 0s 171us/sample - loss: 2.8017e-04 Epoch 84/250 6/6 [==============================] - 0s 327us/sample - loss: 2.7441e-04 Epoch 85/250 6/6 [==============================] - 0s 515us/sample - loss: 2.6878e-04 Epoch 86/250 6/6 [==============================] - 0s 160us/sample - loss: 2.6326e-04 Epoch 87/250 6/6 [==============================] - 0s 174us/sample - loss: 2.5785e-04 Epoch 88/250 6/6 [==============================] - 0s 334us/sample - loss: 2.5255e-04 Epoch 89/250 6/6 [==============================] - 0s 333us/sample - loss: 2.4737e-04 Epoch 90/250 6/6 [==============================] - 0s 534us/sample - loss: 2.4229e-04 Epoch 91/250 6/6 [==============================] - 0s 166us/sample - loss: 2.3731e-04 Epoch 92/250 6/6 [==============================] - 0s 179us/sample - loss: 2.3243e-04 Epoch 93/250 6/6 [==============================] - 0s 373us/sample - loss: 2.2766e-04 Epoch 94/250 6/6 [==============================] - 0s 200us/sample - loss: 2.2298e-04 Epoch 95/250 6/6 [==============================] - 0s 339us/sample - loss: 2.1840e-04 Epoch 96/250 6/6 [==============================] - 0s 358us/sample - loss: 2.1391e-04 Epoch 97/250 6/6 [==============================] - 0s 679us/sample - loss: 2.0952e-04 Epoch 98/250 6/6 [==============================] - 0s 391us/sample - loss: 2.0522e-04 Epoch 99/250 6/6 [==============================] - 0s 332us/sample - loss: 2.0100e-04 Epoch 100/250 6/6 [==============================] - 0s 333us/sample - loss: 1.9687e-04 Epoch 101/250 6/6 [==============================] - 0s 333us/sample - loss: 1.9283e-04 Epoch 102/250 6/6 [==============================] - 0s 188us/sample - loss: 1.8887e-04 Epoch 103/250 6/6 [==============================] - 0s 513us/sample - loss: 1.8499e-04 Epoch 104/250 6/6 [==============================] - 0s 395us/sample - loss: 1.8119e-04 Epoch 105/250 6/6 [==============================] - 0s 543us/sample - loss: 1.7747e-04 Epoch 106/250 6/6 [==============================] - 0s 336us/sample - loss: 1.7382e-04 Epoch 107/250 6/6 [==============================] - 0s 333us/sample - loss: 1.7025e-04 Epoch 108/250 6/6 [==============================] - 0s 166us/sample - loss: 1.6675e-04 Epoch 109/250 6/6 [==============================] - 0s 173us/sample - loss: 1.6333e-04 Epoch 110/250 6/6 [==============================] - 0s 676us/sample - loss: 1.5997e-04 Epoch 111/250 6/6 [==============================] - 0s 175us/sample - loss: 1.5669e-04 Epoch 112/250 6/6 [==============================] - 0s 333us/sample - loss: 1.5347e-04 Epoch 113/250 6/6 [==============================] - 0s 333us/sample - loss: 1.5032e-04 Epoch 114/250 6/6 [==============================] - 0s 180us/sample - loss: 1.4723e-04 Epoch 115/250 6/6 [==============================] - 0s 166us/sample - loss: 1.4420e-04 Epoch 116/250 6/6 [==============================] - 0s 340us/sample - loss: 1.4124e-04 Epoch 117/250 6/6 [==============================] - 0s 673us/sample - loss: 1.3834e-04 Epoch 118/250 6/6 [==============================] - 0s 333us/sample - loss: 1.3550e-04 Epoch 119/250 6/6 [==============================] - 0s 166us/sample - loss: 1.3272e-04 Epoch 120/250 6/6 [==============================] - 0s 331us/sample - loss: 1.2999e-04 Epoch 121/250 6/6 [==============================] - 0s 189us/sample - loss: 1.2732e-04 Epoch 122/250 6/6 [==============================] - 0s 332us/sample - loss: 1.2470e-04 Epoch 123/250 6/6 [==============================] - 0s 333us/sample - loss: 1.2214e-04 Epoch 124/250 6/6 [==============================] - 0s 332us/sample - loss: 1.1963e-04 Epoch 125/250 6/6 [==============================] - 0s 499us/sample - loss: 1.1718e-04 Epoch 126/250 6/6 [==============================] - 0s 665us/sample - loss: 1.1477e-04 Epoch 127/250 6/6 [==============================] - 0s 336us/sample - loss: 1.1241e-04 Epoch 128/250 6/6 [==============================] - 0s 333us/sample - loss: 1.1010e-04 Epoch 129/250 6/6 [==============================] - 0s 337us/sample - loss: 1.0784e-04 Epoch 130/250 6/6 [==============================] - 0s 166us/sample - loss: 1.0563e-04 Epoch 131/250 6/6 [==============================] - 0s 172us/sample - loss: 1.0346e-04 Epoch 132/250 6/6 [==============================] - 0s 336us/sample - loss: 1.0133e-04 Epoch 133/250 6/6 [==============================] - 0s 196us/sample - loss: 9.9251e-05 Epoch 134/250 6/6 [==============================] - 0s 338us/sample - loss: 9.7212e-05 Epoch 135/250 6/6 [==============================] - 0s 374us/sample - loss: 9.5214e-05 Epoch 136/250 6/6 [==============================] - 0s 332us/sample - loss: 9.3259e-05 Epoch 137/250 6/6 [==============================] - 0s 380us/sample - loss: 9.1342e-05 Epoch 138/250 6/6 [==============================] - 0s 186us/sample - loss: 8.9467e-05 Epoch 139/250 6/6 [==============================] - 0s 441us/sample - loss: 8.7628e-05 Epoch 140/250 6/6 [==============================] - 0s 333us/sample - loss: 8.5828e-05 Epoch 141/250 6/6 [==============================] - 0s 166us/sample - loss: 8.4066e-05 Epoch 142/250 6/6 [==============================] - 0s 166us/sample - loss: 8.2339e-05 Epoch 143/250 6/6 [==============================] - 0s 166us/sample - loss: 8.0648e-05 Epoch 144/250 6/6 [==============================] - 0s 180us/sample - loss: 7.8990e-05 Epoch 145/250 6/6 [==============================] - 0s 377us/sample - loss: 7.7368e-05 Epoch 146/250 6/6 [==============================] - 0s 500us/sample - loss: 7.5780e-05 Epoch 147/250 6/6 [==============================] - 0s 332us/sample - loss: 7.4223e-05 Epoch 148/250 6/6 [==============================] - 0s 326us/sample - loss: 7.2699e-05 Epoch 149/250 6/6 [==============================] - 0s 332us/sample - loss: 7.1205e-05 Epoch 150/250 6/6 [==============================] - 0s 333us/sample - loss: 6.9743e-05 Epoch 151/250 6/6 [==============================] - 0s 225us/sample - loss: 6.8309e-05 Epoch 152/250 6/6 [==============================] - 0s 338us/sample - loss: 6.6906e-05 Epoch 153/250 6/6 [==============================] - 0s 332us/sample - loss: 6.5531e-05 Epoch 154/250 6/6 [==============================] - 0s 338us/sample - loss: 6.4187e-05 Epoch 155/250 6/6 [==============================] - 0s 338us/sample - loss: 6.2868e-05 Epoch 156/250 6/6 [==============================] - 0s 361us/sample - loss: 6.1577e-05 Epoch 157/250 6/6 [==============================] - 0s 582us/sample - loss: 6.0313e-05 Epoch 158/250 6/6 [==============================] - 0s 513us/sample - loss: 5.9074e-05 Epoch 159/250 6/6 [==============================] - 0s 181us/sample - loss: 5.7861e-05 Epoch 160/250 6/6 [==============================] - 0s 338us/sample - loss: 5.6672e-05 Epoch 161/250 6/6 [==============================] - 0s 330us/sample - loss: 5.5508e-05 Epoch 162/250 6/6 [==============================] - 0s 338us/sample - loss: 5.4367e-05 Epoch 163/250 6/6 [==============================] - 0s 378us/sample - loss: 5.3251e-05 Epoch 164/250 6/6 [==============================] - 0s 170us/sample - loss: 5.2157e-05 Epoch 165/250 6/6 [==============================] - 0s 327us/sample - loss: 5.1085e-05 Epoch 166/250 6/6 [==============================] - 0s 332us/sample - loss: 5.0036e-05 Epoch 167/250 6/6 [==============================] - 0s 171us/sample - loss: 4.9009e-05 Epoch 168/250 6/6 [==============================] - 0s 333us/sample - loss: 4.8002e-05 Epoch 169/250 6/6 [==============================] - 0s 667us/sample - loss: 4.7016e-05 Epoch 170/250 6/6 [==============================] - 0s 349us/sample - loss: 4.6051e-05 Epoch 171/250 6/6 [==============================] - 0s 332us/sample - loss: 4.5105e-05 Epoch 172/250 6/6 [==============================] - 0s 332us/sample - loss: 4.4178e-05 Epoch 173/250 6/6 [==============================] - 0s 166us/sample - loss: 4.3270e-05 Epoch 174/250 6/6 [==============================] - 0s 332us/sample - loss: 4.2383e-05 Epoch 175/250 6/6 [==============================] - 0s 166us/sample - loss: 4.1511e-05 Epoch 176/250 6/6 [==============================] - 0s 332us/sample - loss: 4.0658e-05 Epoch 177/250 6/6 [==============================] - 0s 166us/sample - loss: 3.9824e-05 Epoch 178/250 6/6 [==============================] - 0s 166us/sample - loss: 3.9005e-05 Epoch 179/250 6/6 [==============================] - 0s 333us/sample - loss: 3.8204e-05 Epoch 180/250 6/6 [==============================] - 0s 499us/sample - loss: 3.7420e-05 Epoch 181/250 6/6 [==============================] - 0s 337us/sample - loss: 3.6651e-05 Epoch 182/250 6/6 [==============================] - 0s 333us/sample - loss: 3.5898e-05 Epoch 183/250 6/6 [==============================] - 0s 332us/sample - loss: 3.5160e-05 Epoch 184/250 6/6 [==============================] - 0s 333us/sample - loss: 3.4438e-05 Epoch 185/250 6/6 [==============================] - 0s 332us/sample - loss: 3.3732e-05 Epoch 186/250 6/6 [==============================] - 0s 197us/sample - loss: 3.3039e-05 Epoch 187/250 6/6 [==============================] - 0s 209us/sample - loss: 3.2359e-05 Epoch 188/250 6/6 [==============================] - 0s 337us/sample - loss: 3.1695e-05 Epoch 189/250 6/6 [==============================] - 0s 333us/sample - loss: 3.1044e-05 Epoch 190/250 6/6 [==============================] - 0s 179us/sample - loss: 3.0406e-05 Epoch 191/250 6/6 [==============================] - 0s 332us/sample - loss: 2.9782e-05 Epoch 192/250 6/6 [==============================] - 0s 339us/sample - loss: 2.9170e-05 Epoch 193/250 6/6 [==============================] - 0s 171us/sample - loss: 2.8571e-05 Epoch 194/250 6/6 [==============================] - 0s 166us/sample - loss: 2.7985e-05 Epoch 195/250 6/6 [==============================] - 0s 406us/sample - loss: 2.7409e-05 Epoch 196/250 6/6 [==============================] - 0s 499us/sample - loss: 2.6846e-05 Epoch 197/250 6/6 [==============================] - 0s 333us/sample - loss: 2.6295e-05 Epoch 198/250 6/6 [==============================] - 0s 332us/sample - loss: 2.5754e-05 Epoch 199/250 6/6 [==============================] - 0s 171us/sample - loss: 2.5226e-05 Epoch 200/250 6/6 [==============================] - 0s 177us/sample - loss: 2.4707e-05 Epoch 201/250 6/6 [==============================] - 0s 339us/sample - loss: 2.4199e-05 Epoch 202/250 6/6 [==============================] - 0s 332us/sample - loss: 2.3702e-05 Epoch 203/250 6/6 [==============================] - 0s 332us/sample - loss: 2.3216e-05 Epoch 204/250 6/6 [==============================] - 0s 192us/sample - loss: 2.2738e-05 Epoch 205/250 6/6 [==============================] - 0s 333us/sample - loss: 2.2272e-05 Epoch 206/250 6/6 [==============================] - 0s 332us/sample - loss: 2.1814e-05 Epoch 207/250 6/6 [==============================] - 0s 336us/sample - loss: 2.1366e-05 Epoch 208/250 6/6 [==============================] - 0s 332us/sample - loss: 2.0927e-05 Epoch 209/250 6/6 [==============================] - 0s 535us/sample - loss: 2.0496e-05 Epoch 210/250 6/6 [==============================] - 0s 333us/sample - loss: 2.0076e-05 Epoch 211/250 6/6 [==============================] - 0s 167us/sample - loss: 1.9663e-05 Epoch 212/250 6/6 [==============================] - 0s 342us/sample - loss: 1.9260e-05 Epoch 213/250 6/6 [==============================] - 0s 216us/sample - loss: 1.8864e-05 Epoch 214/250 6/6 [==============================] - 0s 332us/sample - loss: 1.8476e-05 Epoch 215/250 6/6 [==============================] - 0s 333us/sample - loss: 1.8097e-05 Epoch 216/250 6/6 [==============================] - 0s 341us/sample - loss: 1.7725e-05 Epoch 217/250 6/6 [==============================] - 0s 333us/sample - loss: 1.7361e-05 Epoch 218/250 6/6 [==============================] - 0s 168us/sample - loss: 1.7004e-05 Epoch 219/250 6/6 [==============================] - 0s 332us/sample - loss: 1.6655e-05 Epoch 220/250 6/6 [==============================] - 0s 166us/sample - loss: 1.6313e-05 Epoch 221/250 6/6 [==============================] - 0s 166us/sample - loss: 1.5978e-05 Epoch 222/250 6/6 [==============================] - 0s 391us/sample - loss: 1.5650e-05 Epoch 223/250 6/6 [==============================] - 0s 500us/sample - loss: 1.5328e-05 Epoch 224/250 6/6 [==============================] - 0s 219us/sample - loss: 1.5013e-05 Epoch 225/250 6/6 [==============================] - 0s 327us/sample - loss: 1.4705e-05 Epoch 226/250 6/6 [==============================] - 0s 330us/sample - loss: 1.4403e-05 Epoch 227/250 6/6 [==============================] - 0s 333us/sample - loss: 1.4107e-05 Epoch 228/250 6/6 [==============================] - 0s 179us/sample - loss: 1.3818e-05 Epoch 229/250 6/6 [==============================] - 0s 333us/sample - loss: 1.3533e-05 Epoch 230/250 6/6 [==============================] - 0s 333us/sample - loss: 1.3256e-05 Epoch 231/250 6/6 [==============================] - 0s 333us/sample - loss: 1.2983e-05 Epoch 232/250 6/6 [==============================] - 0s 166us/sample - loss: 1.2717e-05 Epoch 233/250 6/6 [==============================] - 0s 193us/sample - loss: 1.2456e-05 Epoch 234/250 6/6 [==============================] - 0s 166us/sample - loss: 1.2200e-05 Epoch 235/250 6/6 [==============================] - 0s 338us/sample - loss: 1.1949e-05 Epoch 236/250 6/6 [==============================] - 0s 493us/sample - loss: 1.1703e-05 Epoch 237/250 6/6 [==============================] - 0s 338us/sample - loss: 1.1463e-05 Epoch 238/250 6/6 [==============================] - 0s 166us/sample - loss: 1.1228e-05 Epoch 239/250 6/6 [==============================] - 0s 499us/sample - loss: 1.0997e-05 Epoch 240/250 6/6 [==============================] - 0s 359us/sample - loss: 1.0771e-05 Epoch 241/250 6/6 [==============================] - 0s 166us/sample - loss: 1.0550e-05 Epoch 242/250 6/6 [==============================] - 0s 166us/sample - loss: 1.0333e-05 Epoch 243/250 6/6 [==============================] - 0s 332us/sample - loss: 1.0121e-05 Epoch 244/250 6/6 [==============================] - 0s 332us/sample - loss: 9.9131e-06 Epoch 245/250 6/6 [==============================] - 0s 169us/sample - loss: 9.7095e-06 Epoch 246/250 6/6 [==============================] - 0s 333us/sample - loss: 9.5102e-06 Epoch 247/250 6/6 [==============================] - 0s 346us/sample - loss: 9.3147e-06 Epoch 248/250 6/6 [==============================] - 0s 166us/sample - loss: 9.1239e-06 Epoch 249/250 6/6 [==============================] - 0s 333us/sample - loss: 8.9360e-06 Epoch 250/250 6/6 [==============================] - 0s 166us/sample - loss: 8.7528e-06 . &lt;tensorflow.python.keras.callbacks.History at 0x221dd2a8d30&gt; . . Making a Prediction . print(model.predict([10])) . [[18.991367]] . Seeing what the network learned . print(f&quot;Here&#39;s what I learned: {layer0.get_weights()}&quot;) . Here&#39;s what I learned: [array([[1.9987489]], dtype=float32), array([-0.99612147], dtype=float32)] . print(f&quot;Thus the learned Values were : Y = {layer0.get_weights()[0][0][0]}X - {layer0.get_weights()[1][0]}&quot;) . Thus the learned Values were : Y = 1.9987488985061646X - -0.9961214661598206 .",
            "url": "https://astronaut99.github.io/ml-blog/2021/09/28/The-Beginning.html",
            "relUrl": "/2021/09/28/The-Beginning.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://astronaut99.github.io/ml-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://astronaut99.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://astronaut99.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}